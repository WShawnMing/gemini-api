"""
Gemini Web API æœåŠ¡
åŸºäº FastAPI çš„ RESTful APIï¼Œæä¾› Google Gemini çš„å®Œæ•´åŠŸèƒ½
"""
import asyncio
import os
import tempfile
import shutil
import time
import uuid
import json
import hashlib
from typing import Optional, List, Dict, Any
from pathlib import Path
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.exceptions import RequestValidationError
from pydantic import BaseModel, Field
import uvicorn

from gemini_webapi import GeminiClient, set_log_level
from gemini_webapi.constants import Model

# ==================== æ—¥å¿—é…ç½® ====================
# è®¾ç½® gemini_webapi æ—¥å¿—çº§åˆ«ä¸º INFOï¼Œå‡å°‘ DEBUG å™ªéŸ³
# å¯é€‰å€¼: DEBUG, INFO, WARNING, ERROR, CRITICAL
set_log_level("INFO")

# ==================== é…ç½® ====================
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
    secure_1psid = os.getenv("GEMINI_1PSID")
    secure_1psidts = os.getenv("GEMINI_1PSIDTS")
    
    # å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
    if not secure_1psid or not secure_1psidts:
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    secure_1psid = secure_1psid or config.get("gemini", {}).get("secure_1psid", "")
                    secure_1psidts = secure_1psidts or config.get("gemini", {}).get("secure_1psidts", "")
                    proxy = config.get("gemini", {}).get("proxy")
            except Exception as e:
                print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                secure_1psid = secure_1psid or ""
                secure_1psidts = secure_1psidts or ""
                proxy = None
        else:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            print(f"ğŸ’¡ è¯·åˆ›å»º config.json æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_1PSID å’Œ GEMINI_1PSIDTS")
            secure_1psid = secure_1psid or ""
            secure_1psidts = secure_1psidts or ""
            proxy = None
    else:
        proxy = None
    
    # è¯»å–æœåŠ¡å™¨é…ç½®
    server_config = {
        "host": "0.0.0.0",
        "port": 8000,
        "log_level": "info"
    }
    client_config = {
        "timeout": 30,
        "auto_close": False,
        "close_delay": 300,
        "auto_refresh": True
    }
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                server_config.update(config.get("server", {}))
                client_config.update(config.get("client", {}))
                if proxy is None:
                    proxy = config.get("gemini", {}).get("proxy")
        except:
            pass
    
    return {
        "secure_1psid": secure_1psid,
        "secure_1psidts": secure_1psidts,
        "proxy": proxy,
        "server": server_config,
        "client": client_config
    }

# åŠ è½½é…ç½®
config = load_config()
Secure_1PSID = config["secure_1psid"]
Secure_1PSIDTS = config["secure_1psidts"]
Proxy = config["proxy"]
ServerConfig = config["server"]
ClientConfig = config["client"]

# ==================== å…¨å±€å˜é‡ ====================
client: Optional[GeminiClient] = None
chat_sessions: Dict[str, Any] = {}  # å­˜å‚¨ä¼šè¯å¯¹è±¡ï¼ˆç”¨äº /chat/session ç«¯ç‚¹ï¼‰
openai_sessions: Dict[str, Any] = {}  # å­˜å‚¨ OpenAI æ ¼å¼çš„ä¼šè¯ï¼ˆç”¨äº /v1/chat/completionsï¼‰


# ==================== ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–å®¢æˆ·ç«¯
    global client
    try:
        client = GeminiClient(Secure_1PSID, Secure_1PSIDTS, proxy=Proxy)
        await client.init(
            timeout=ClientConfig["timeout"],
            auto_close=ClientConfig["auto_close"],
            close_delay=ClientConfig["close_delay"],
            auto_refresh=ClientConfig["auto_refresh"]
        )
        print("âœ… Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    if client:
        try:
            await client.close()
            print("âœ… Gemini å®¢æˆ·ç«¯å·²å…³é—­")
        except Exception as e:
            print(f"âš ï¸ å…³é—­å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")


# ==================== FastAPI åº”ç”¨ ====================
app = FastAPI(
    title="Gemini Web API",
    description="åŸºäº gemini-webapi çš„ RESTful API æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== é”™è¯¯å¤„ç† ====================
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """å¤„ç†è¯·æ±‚éªŒè¯é”™è¯¯"""
    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": "è¯·æ±‚æ ¼å¼é”™è¯¯",
                "type": "invalid_request_error",
                "param": None,
                "code": None,
                "details": exc.errors()
            }
        }
    )



# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================
class ChatRequest(BaseModel):
    """å•æ¬¡å¯¹è¯è¯·æ±‚"""
    message: str = Field(..., description="è¦å‘é€çš„æ¶ˆæ¯")
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°ï¼Œå¦‚ gemini-2.5-pro")
    gem: Optional[str] = Field(None, description="Gemini Gem ID")


# OpenAI å…¼å®¹æ ¼å¼çš„è¯·æ±‚/å“åº”æ¨¡å‹
class OpenAIMessage(BaseModel):
    """OpenAI æ¶ˆæ¯æ ¼å¼"""
    role: str = Field(..., description="è§’è‰²: user, assistant, system")
    content: Any = Field(..., description="æ¶ˆæ¯å†…å®¹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰")
    
    def get_text_content(self) -> str:
        """è·å–æ–‡æœ¬å†…å®¹"""
        if isinstance(self.content, str):
            return self.content
        elif isinstance(self.content, list):
            # å¤„ç†å†…å®¹å—åˆ—è¡¨
            text_parts = []
            for item in self.content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                    elif "text" in item:
                        text_parts.append(item["text"])
                elif isinstance(item, str):
                    text_parts.append(item)
            return " ".join(text_parts)
        else:
            return str(self.content)


class OpenAICompletionRequest(BaseModel):
    """OpenAI Chat Completions è¯·æ±‚æ ¼å¼"""
    model: Optional[str] = Field(default="gemini-2.5-flash", description="æ¨¡å‹åç§°")
    messages: List[OpenAIMessage] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    temperature: Optional[float] = Field(default=None, description="æ¸©åº¦å‚æ•°")
    max_tokens: Optional[int] = Field(default=None, description="æœ€å¤§tokenæ•°")
    stream: Optional[bool] = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")
    
    class Config:
        # å…è®¸é¢å¤–å­—æ®µï¼Œæé«˜å…¼å®¹æ€§
        extra = "allow"


class OpenAIChoice(BaseModel):
    """OpenAI Choice æ ¼å¼"""
    index: int
    message: OpenAIMessage
    finish_reason: str = "stop"


class Usage(BaseModel):
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0


class OpenAICompletionResponse(BaseModel):
    """OpenAI Chat Completions å“åº”æ ¼å¼"""
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[OpenAIChoice]
    usage: Optional[Usage] = None


class ChatResponse(BaseModel):
    """å¯¹è¯å“åº”"""
    text: str = Field(..., description="ç”Ÿæˆçš„æ–‡æœ¬")
    images: Optional[List[Dict[str, Any]]] = Field(None, description="å›¾ç‰‡åˆ—è¡¨")
    thoughts: Optional[str] = Field(None, description="æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹")
    candidates_count: Optional[int] = Field(None, description="å€™é€‰å›å¤æ•°é‡")
    metadata: Optional[Dict[str, Any]] = Field(None, description="ä¼šè¯å…ƒæ•°æ®")


class SessionCreateRequest(BaseModel):
    """åˆ›å»ºä¼šè¯è¯·æ±‚"""
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°")
    gem: Optional[str] = Field(None, description="Gemini Gem ID")
    metadata: Optional[Dict[str, Any]] = Field(None, description="æ¢å¤ä¼šè¯çš„å…ƒæ•°æ®")


class SessionMessageRequest(BaseModel):
    """ä¼šè¯æ¶ˆæ¯è¯·æ±‚"""
    message: str = Field(..., description="è¦å‘é€çš„æ¶ˆæ¯")


class SessionResponse(BaseModel):
    """ä¼šè¯å“åº”"""
    session_id: str = Field(..., description="ä¼šè¯ID")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")


class GemCreateRequest(BaseModel):
    """åˆ›å»º Gem è¯·æ±‚"""
    name: str = Field(..., description="Gem åç§°")
    prompt: str = Field(..., description="ç³»ç»Ÿæç¤ºè¯")
    description: Optional[str] = Field(None, description="Gem æè¿°")


class GemUpdateRequest(BaseModel):
    """æ›´æ–° Gem è¯·æ±‚"""
    name: str = Field(..., description="Gem åç§°")
    prompt: str = Field(..., description="ç³»ç»Ÿæç¤ºè¯")
    description: Optional[str] = Field(None, description="Gem æè¿°")


# ç”Ÿå‘½å‘¨æœŸäº‹ä»¶å·²ç§»è‡³ lifespan ä¸Šä¸‹æ–‡ç®¡ç†å™¨


# ==================== å·¥å…·å‡½æ•° ====================
def format_response(response) -> ChatResponse:
    """æ ¼å¼åŒ–å“åº”å¯¹è±¡"""
    images = None
    if response.images:
        images = []
        for img in response.images:
            img_dict = {
                "title": getattr(img, 'title', None),
                "url": getattr(img, 'url', None),
                "alt": getattr(img, 'alt', None),
            }
            # æ·»åŠ å›¾ç‰‡ç±»å‹
            img_type = type(img).__name__
            img_dict["type"] = img_type
            images.append(img_dict)
    
    metadata = None
    if hasattr(response, 'metadata'):
        metadata = response.metadata
    
    return ChatResponse(
        text=response.text,
        images=images,
        thoughts=getattr(response, 'thoughts', None),
        candidates_count=len(response.candidates) if hasattr(response, 'candidates') else None,
        metadata=metadata
    )


# ==================== API ç«¯ç‚¹ ====================
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "Gemini Web API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "client_initialized": client is not None,
        "active_sessions": len(chat_sessions)
    }


@app.post("/v1/chat/completions/debug")
async def chat_completions_debug(request: OpenAICompletionRequest):
    """
    è°ƒè¯•ç«¯ç‚¹ - è¿”å›è¯¦ç»†çš„å“åº”ä¿¡æ¯ï¼Œç”¨äºè¯Šæ–­ Chatbox é—®é¢˜
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        import time
        import uuid
        
        # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå•ä¸ªæ¶ˆæ¯ï¼ˆå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if not user_messages:
            return {
                "error": "è‡³å°‘éœ€è¦ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯"
            }
        
        message_content = user_messages[-1].get_text_content()
        
        # è°ƒç”¨ Gemini API
        kwargs = {}
        if request.model:
            kwargs["model"] = request.model
        
        response = await client.generate_content(message_content, **kwargs)
        response_text = response.text if response and hasattr(response, 'text') and response.text else ""
        
        # ä¼°ç®— token
        prompt_tokens = len(message_content.encode('utf-8')) // 2
        completion_tokens = len(response_text.encode('utf-8')) // 2 if response_text else 0
        total_tokens = prompt_tokens + completion_tokens
        model_name = request.model or "gemini-2.5-flash"
        
        # æ„å»ºå“åº”
        response_data = {
            "id": f"chatcmpl-{uuid.uuid4().hex[:29]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model_name,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text if response_text else "æ— å†…å®¹"
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens
            }
        }
        
        # è¿”å›è°ƒè¯•ä¿¡æ¯
        return {
            "debug": {
                "response_text_length": len(response_text),
                "response_text_type": type(response_text).__name__,
                "response_text_preview": response_text[:100] if response_text else "ç©º",
                "has_content": bool(response_text),
                "usage_calculated": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens
                }
            },
            "response": response_data,
            "raw_response_attributes": {
                "has_text": hasattr(response, 'text'),
                "text_value": str(response.text) if hasattr(response, 'text') else "N/A",
                "has_candidates": hasattr(response, 'candidates'),
                "candidates_count": len(response.candidates) if hasattr(response, 'candidates') else 0
            }
        }
    except Exception as e:
        return {
            "error": str(e),
            "error_type": type(e).__name__
        }


@app.post("/v1/chat/completions", response_model=None)
async def chat_completions(request: OpenAICompletionRequest, http_request: Request):
    """
    OpenAI å…¼å®¹æ ¼å¼çš„å¯¹è¯æ¥å£
    æ”¯æŒæ ‡å‡†çš„ OpenAI API æ ¼å¼ï¼Œå¯ç”¨äº Chatbox ç­‰å®¢æˆ·ç«¯
    æ”¯æŒæµå¼å’Œéæµå¼å“åº”
    æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆå¤šè½®å¯¹è¯ï¼‰
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        import time
        import uuid
        import hashlib
        
        # æ£€æŸ¥æ¶ˆæ¯åˆ—è¡¨
        if not request.messages:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": {
                        "message": "æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º",
                        "type": "invalid_request_error"
                    }
                }
            )
        
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if not user_messages:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": {
                        "message": "è‡³å°‘éœ€è¦ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯",
                        "type": "invalid_request_error"
                    }
                }
            )
        
        # ä½¿ç”¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºå½“å‰è¾“å…¥
        current_user_message = user_messages[-1].get_text_content()
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨ä¼šè¯ï¼ˆå¦‚æœæ¶ˆæ¯å†å²ä¸­æœ‰ assistant æ¶ˆæ¯ï¼Œè¯´æ˜æ˜¯ç»§ç»­å¯¹è¯ï¼‰
        has_assistant_message = any(msg.role == "assistant" for msg in request.messages)
        use_session = has_assistant_message and len(request.messages) > 1
        
        # è·å–æˆ–åˆ›å»ºä¼šè¯
        session_key = None
        chat = None
        
        if use_session:
            # å°è¯•ä»è¯·æ±‚å¤´è·å–ä¼šè¯IDï¼ˆå¦‚æœå®¢æˆ·ç«¯æä¾›äº†ï¼‰
            session_id_header = http_request.headers.get("X-Session-ID")
            
            if session_id_header:
                session_key = session_id_header
            else:
                # å¦‚æœæ²¡æœ‰æä¾›ä¼šè¯IDï¼ŒåŸºäºæ¨¡å‹ç”Ÿæˆä¸€ä¸ªå›ºå®šçš„ä¼šè¯é”®
                # è¿™æ ·åŒä¸€ä¸ªæ¨¡å‹çš„å¯¹è¯ä¼šä½¿ç”¨åŒä¸€ä¸ªä¼šè¯
                model_name = request.model or "gemini-2.5-flash"
                session_key = f"openai_session_{model_name}"
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¼šè¯
            if session_key in openai_sessions:
                chat = openai_sessions[session_key]["chat"]
                # éªŒè¯ä¼šè¯æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                try:
                    # å°è¯•è®¿é—®ä¼šè¯å±æ€§æ¥éªŒè¯
                    _ = chat.metadata if hasattr(chat, 'metadata') else None
                except:
                    # ä¼šè¯å·²å¤±æ•ˆï¼Œåˆ›å»ºæ–°ä¼šè¯
                    chat = None
                    del openai_sessions[session_key]
            
            # æ£€æŸ¥ä¼šè¯ä¸­å·²æœ‰çš„æ¶ˆæ¯æ•°é‡
            existing_message_count = openai_sessions.get(session_key, {}).get("message_count", 0) if session_key in openai_sessions else 0
            history_user_messages = [m for m in request.messages[:-1] if m.role == "user"]
            history_count = len(history_user_messages)
            
            if not chat:
                # åˆ›å»ºæ–°ä¼šè¯
                kwargs = {}
                if request.model:
                    kwargs["model"] = request.model
                
                chat = client.start_chat(**kwargs)
                
                # å¦‚æœæ˜¯æ–°ä¼šè¯ä¸”æœ‰å†å²æ¶ˆæ¯ï¼Œéœ€è¦å°†å†å²æ¶ˆæ¯å‘é€ç»™ Gemini
                # è¿™æ · Gemini æ‰èƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯
                if history_count > 0:
                    system_messages = [m for m in request.messages if m.role == "system"]
                    system_prompt = "\n".join([m.get_text_content() for m in system_messages]) if system_messages else None
                    
                    # éå†å†å²æ¶ˆæ¯ï¼Œæˆå¯¹å‘é€ user-assistant
                    i = 0
                    user_msg_index = 0
                    while i < len(request.messages) - 1:  # æ’é™¤æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                        msg = request.messages[i]
                        if msg.role == "user":
                            # å‘é€ç”¨æˆ·æ¶ˆæ¯
                            user_content = msg.get_text_content()
                            # å¦‚æœæ˜¯ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸”æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
                            if user_msg_index == 0 and system_prompt:
                                user_content = f"{system_prompt}\n\n{user_content}"
                            
                            # å‘é€æ¶ˆæ¯ï¼ŒGemini ä¼šç”Ÿæˆå›å¤
                            await chat.send_message(user_content)
                            user_msg_index += 1
                            
                            # è·³è¿‡ä¸‹ä¸€æ¡ assistant æ¶ˆæ¯ï¼ˆå› ä¸º Gemini å·²ç»ç”Ÿæˆäº†ï¼‰
                            if i + 1 < len(request.messages) - 1 and request.messages[i + 1].role == "assistant":
                                i += 2
                            else:
                                i += 1
                        else:
                            i += 1
                
                openai_sessions[session_key] = {
                    "chat": chat,
                    "model": request.model or "gemini-2.5-flash",
                    "created": int(time.time()),
                    "message_count": history_count
                }
            elif existing_message_count < history_count:
                # ä¼šè¯å­˜åœ¨ä½†æ¶ˆæ¯æ•°é‡ä¸è¶³ï¼Œéœ€è¦è¡¥å……å†å²æ¶ˆæ¯
                # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¦‚æœå‘ç”Ÿäº†ï¼Œè¡¥å……ç¼ºå¤±çš„æ¶ˆæ¯
                missing_count = history_count - existing_message_count
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šé‡æ–°å‘é€æ‰€æœ‰å†å²æ¶ˆæ¯ï¼ˆå®é™…åº”è¯¥åªå‘é€ç¼ºå¤±çš„éƒ¨åˆ†ï¼‰
                # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶è·³è¿‡è¿™ä¸ªå¤æ‚é€»è¾‘
                pass
            
            # æ›´æ–°æ¶ˆæ¯è®¡æ•°ï¼ˆå½“å‰æ¶ˆæ¯ä¼šåœ¨ä¸‹é¢å‘é€ï¼‰
            if session_key in openai_sessions:
                openai_sessions[session_key]["message_count"] = history_count + 1
        
        # å‡†å¤‡æ¶ˆæ¯å†…å®¹
        message_content = current_user_message
        
        # è°ƒç”¨ Gemini API
        if use_session and chat:
            # ä½¿ç”¨ä¼šè¯å‘é€æ¶ˆæ¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
            # æ³¨æ„ï¼šå¦‚æœä¼šè¯æ˜¯æ–°åˆ›å»ºçš„ï¼Œå†å²æ¶ˆæ¯å·²ç»åœ¨ä¸Šé¢å‘é€è¿‡äº†
            # è¿™é‡Œåªéœ€è¦å‘é€å½“å‰ç”¨æˆ·æ¶ˆæ¯
            response = await chat.send_message(message_content)
        else:
            # å•æ¬¡å¯¹è¯ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰
            # å¦‚æœæœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
            system_messages = [msg for msg in request.messages if msg.role == "system"]
            if system_messages:
                system_prompt = "\n".join([msg.get_text_content() for msg in system_messages])
                message_content = f"{system_prompt}\n\n{message_content}"
            
            kwargs = {}
            if request.model:
                kwargs["model"] = request.model
            response = await client.generate_content(message_content, **kwargs)
        
        # ç¡®ä¿å“åº”æ–‡æœ¬ä¸ä¸ºç©º
        response_text = response.text if response and hasattr(response, 'text') and response.text else ""
        
        # å¦‚æœå“åº”ä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–å±æ€§è·å–
        if not response_text:
            # å°è¯•ä» candidates è·å–
            if hasattr(response, 'candidates') and response.candidates:
                first_candidate = response.candidates[0]
                if hasattr(first_candidate, 'text'):
                    response_text = first_candidate.text
                elif hasattr(first_candidate, 'content'):
                    response_text = str(first_candidate.content)
        
        # å¦‚æœä»ç„¶ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯
        if not response_text:
            response_text = "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›å¤å†…å®¹ã€‚"
        
        # ä¼°ç®— token æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æŒ‰2ä¸ªtokenï¼Œè‹±æ–‡æŒ‰1ä¸ªtokenï¼‰
        prompt_tokens = len(message_content.encode('utf-8')) // 2  # ç®€å•ä¼°ç®—
        completion_tokens = len(response_text.encode('utf-8')) // 2
        total_tokens = prompt_tokens + completion_tokens
        
        # ç¡®ä¿ model å­—æ®µä¸ä¸ºç©º
        model_name = request.model or "gemini-2.5-flash"
        response_id = f"chatcmpl-{uuid.uuid4().hex[:29]}"
        created_time = int(time.time())
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æµå¼å“åº”
        if request.stream:
            # æµå¼å“åº”ï¼ˆSSE æ ¼å¼ï¼‰
            import json as json_lib
            async def generate_stream():
                # å‘é€åˆå§‹æ•°æ®
                initial_data = {
                    'id': response_id,
                    'object': 'chat.completion.chunk',
                    'created': created_time,
                    'model': model_name,
                    'choices': [{'index': 0, 'delta': {'role': 'assistant', 'content': ''}, 'finish_reason': None}]
                }
                yield f"data: {json_lib.dumps(initial_data, ensure_ascii=False)}\n\n"
                
                # é€å­—ç¬¦å‘é€å†…å®¹ï¼ˆæ¨¡æ‹Ÿæµå¼ï¼‰
                for char in response_text:
                    chunk_data = {
                        'id': response_id,
                        'object': 'chat.completion.chunk',
                        'created': created_time,
                        'model': model_name,
                        'choices': [{'index': 0, 'delta': {'content': char}, 'finish_reason': None}]
                    }
                    yield f"data: {json_lib.dumps(chunk_data, ensure_ascii=False)}\n\n"
                
                # å‘é€ç»“æŸæ ‡è®°
                final_data = {
                    'id': response_id,
                    'object': 'chat.completion.chunk',
                    'created': created_time,
                    'model': model_name,
                    'choices': [{'index': 0, 'delta': {}, 'finish_reason': 'stop'}]
                }
                yield f"data: {json_lib.dumps(final_data, ensure_ascii=False)}\n\n"
                
                # å‘é€ usage
                usage_data = {
                    'id': response_id,
                    'object': 'chat.completion.chunk',
                    'created': created_time,
                    'model': model_name,
                    'choices': [],
                    'usage': {
                        'prompt_tokens': prompt_tokens,
                        'completion_tokens': completion_tokens,
                        'total_tokens': total_tokens
                    }
                }
                yield f"data: {json_lib.dumps(usage_data, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                }
            )
        else:
            # éæµå¼å“åº”ï¼ˆæ ‡å‡† JSONï¼‰
            response_data = {
                "id": response_id,
                "object": "chat.completion",
                "created": created_time,
                "model": model_name,
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": response_text
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens
                }
            }
            
            # è¿”å› JSONResponse ç¡®ä¿æ­£ç¡®çš„ Content-Type
            return JSONResponse(
                content=response_data,
                headers={
                    "Content-Type": "application/json; charset=utf-8"
                }
            )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå†…å®¹æ—¶å‡ºé”™: {str(e)}")


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    å•æ¬¡å¯¹è¯æ¥å£
    ä¸ä¿å­˜å†å²è®°å½•ï¼Œæ¯æ¬¡éƒ½æ˜¯ç‹¬ç«‹å¯¹è¯
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        kwargs = {}
        if request.model:
            kwargs["model"] = request.model
        if request.gem:
            kwargs["gem"] = request.gem
        
        response = await client.generate_content(request.message, **kwargs)
        return format_response(response)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå†…å®¹æ—¶å‡ºé”™: {str(e)}")


@app.post("/chat/with-files", response_model=ChatResponse)
async def chat_with_files(
    message: str = Form(...),
    files: List[UploadFile] = File(None),
    model: Optional[str] = Form(None),
    gem: Optional[str] = Form(None)
):
    """
    å¸¦æ–‡ä»¶ä¸Šä¼ çš„å•æ¬¡å¯¹è¯
    æ”¯æŒå›¾ç‰‡å’Œæ–‡æ¡£æ–‡ä»¶
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    temp_dir = None
    try:
        file_paths = []
        if files:
            temp_dir = tempfile.mkdtemp()
            for file in files:
                file_path = os.path.join(temp_dir, file.filename)
                with open(file_path, "wb") as f:
                    content = await file.read()
                    f.write(content)
                file_paths.append(file_path)
        
        kwargs = {}
        if model:
            kwargs["model"] = model
        if gem:
            kwargs["gem"] = gem
        if file_paths:
            kwargs["files"] = file_paths
        
        response = await client.generate_content(message, **kwargs)
        return format_response(response)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå†…å®¹æ—¶å‡ºé”™: {str(e)}")
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_dir and os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)


@app.post("/chat/session", response_model=SessionResponse)
async def create_session(request: SessionCreateRequest):
    """
    åˆ›å»ºæ–°çš„å¯¹è¯ä¼šè¯
    ä¼šè¯ä¼šä¿å­˜å†å²è®°å½•ï¼Œæ”¯æŒå¤šè½®å¯¹è¯
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        kwargs = {}
        if request.model:
            kwargs["model"] = request.model
        if request.gem:
            kwargs["gem"] = request.gem
        if request.metadata:
            kwargs["metadata"] = request.metadata
        
        chat = client.start_chat(**kwargs)
        session_id = str(id(chat))
        chat_sessions[session_id] = {
            "chat": chat,
            "metadata": chat.metadata if hasattr(chat, 'metadata') else None
        }
        
        return SessionResponse(
            session_id=session_id,
            message="ä¼šè¯åˆ›å»ºæˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¼šè¯æ—¶å‡ºé”™: {str(e)}")


@app.post("/chat/session/{session_id}", response_model=ChatResponse)
async def session_message(session_id: str, request: SessionMessageRequest):
    """
    å‘æŒ‡å®šä¼šè¯å‘é€æ¶ˆæ¯
    ä¼šè¯ä¼šä¿æŒå†å²è®°å½•
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    if session_id not in chat_sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    try:
        chat = chat_sessions[session_id]["chat"]
        response = await chat.send_message(request.message)
        
        # æ›´æ–°ä¼šè¯å…ƒæ•°æ®
        if hasattr(response, 'metadata'):
            chat_sessions[session_id]["metadata"] = response.metadata
        
        return format_response(response)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")


@app.get("/chat/session/{session_id}/metadata")
async def get_session_metadata(session_id: str):
    """è·å–ä¼šè¯å…ƒæ•°æ®"""
    if session_id not in chat_sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    return {
        "session_id": session_id,
        "metadata": chat_sessions[session_id].get("metadata")
    }


@app.delete("/chat/session/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤æŒ‡å®šçš„ä¼šè¯"""
    if session_id in chat_sessions:
        del chat_sessions[session_id]
        return {"message": "ä¼šè¯å·²åˆ é™¤"}
    else:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")


@app.get("/chat/sessions")
async def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰æ´»è·ƒçš„ä¼šè¯"""
    sessions = []
    for session_id, session_data in chat_sessions.items():
        sessions.append({
            "session_id": session_id,
            "metadata": session_data.get("metadata")
        })
    
    return {
        "sessions": sessions,
        "count": len(sessions)
    }


@app.post("/chat/session/{session_id}/choose-candidate")
async def choose_candidate(session_id: str, index: int = 0):
    """
    é€‰æ‹©ä¼šè¯ä¸­çš„å€™é€‰å›å¤
    """
    if session_id not in chat_sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    try:
        chat = chat_sessions[session_id]["chat"]
        candidate = chat.choose_candidate(index=index)
        return {
            "message": "å€™é€‰å›å¤å·²é€‰æ‹©",
            "candidate_text": candidate.text if hasattr(candidate, 'text') else str(candidate)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é€‰æ‹©å€™é€‰å›å¤æ—¶å‡ºé”™: {str(e)}")


# ==================== Gems ç®¡ç† ====================
@app.get("/gems")
async def list_gems(include_hidden: bool = False):
    """
    è·å–æ‰€æœ‰ Gemsï¼ˆç³»ç»Ÿæç¤ºè¯ï¼‰
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        await client.fetch_gems(include_hidden=include_hidden)
        gems = client.gems
        
        gems_list = []
        for gem in gems:
            gems_list.append({
                "id": gem.id,
                "name": gem.name,
                "description": getattr(gem, 'description', None),
                "predefined": getattr(gem, 'predefined', False)
            })
        
        return {
            "gems": gems_list,
            "count": len(gems_list)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å– Gems æ—¶å‡ºé”™: {str(e)}")


@app.post("/gems")
async def create_gem(request: GemCreateRequest):
    """
    åˆ›å»ºè‡ªå®šä¹‰ Gem
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        new_gem = await client.create_gem(
            name=request.name,
            prompt=request.prompt,
            description=request.description
        )
        
        return {
            "id": new_gem.id,
            "name": new_gem.name,
            "description": getattr(new_gem, 'description', None),
            "message": "Gem åˆ›å»ºæˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»º Gem æ—¶å‡ºé”™: {str(e)}")


@app.put("/gems/{gem_id}")
async def update_gem(gem_id: str, request: GemUpdateRequest):
    """
    æ›´æ–°è‡ªå®šä¹‰ Gem
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        updated_gem = await client.update_gem(
            gem=gem_id,
            name=request.name,
            prompt=request.prompt,
            description=request.description
        )
        
        return {
            "id": updated_gem.id,
            "name": updated_gem.name,
            "description": getattr(updated_gem, 'description', None),
            "message": "Gem æ›´æ–°æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–° Gem æ—¶å‡ºé”™: {str(e)}")


@app.delete("/gems/{gem_id}")
async def delete_gem(gem_id: str):
    """
    åˆ é™¤è‡ªå®šä¹‰ Gem
    """
    if not client:
        raise HTTPException(status_code=503, detail="å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        await client.delete_gem(gem_id)
        return {"message": "Gem åˆ é™¤æˆåŠŸ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ Gem æ—¶å‡ºé”™: {str(e)}")


# ==================== æ¨¡å‹ä¿¡æ¯ ====================
@app.get("/models")
async def list_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return {
        "models": [
            {
                "id": "unspecified",
                "name": "é»˜è®¤æ¨¡å‹",
                "description": "Gemini é»˜è®¤æ¨¡å‹"
            },
            {
                "id": "gemini-3.0-pro",
                "name": "Gemini 3.0 Pro",
                "description": "Gemini 3.0 Pro æ¨¡å‹"
            },
            {
                "id": "gemini-2.5-pro",
                "name": "Gemini 2.5 Pro",
                "description": "Gemini 2.5 Pro æ¨¡å‹"
            },
            {
                "id": "gemini-2.5-flash",
                "name": "Gemini 2.5 Flash",
                "description": "Gemini 2.5 Flash æ¨¡å‹ï¼ˆå¿«é€Ÿï¼‰"
            }
        ]
    }


if __name__ == "__main__":
    # é…ç½® uvicorn æ—¥å¿—çº§åˆ«
    # å¯é€‰å€¼: critical, error, warning, info, debug, trace
    log_level = os.getenv("LOG_LEVEL", ServerConfig.get("log_level", "info")).lower()
    
    uvicorn.run(
        app,
        host=ServerConfig.get("host", "0.0.0.0"),
        port=ServerConfig.get("port", 8000),
        log_level=log_level
    )

